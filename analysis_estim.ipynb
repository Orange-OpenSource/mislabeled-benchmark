{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software Name : mislabeled-benchmark\n",
    "# SPDX-FileCopyrightText: Copyright (c) Orange Innovation\n",
    "# SPDX-License-Identifier: MIT\n",
    "# \n",
    "# This software is distributed under the MIT license,\n",
    "# see the \"LICENSE.md\" file for more details\n",
    "# or https://github.com/Orange-OpenSource/mislabeled-benchmark/blob/master/LICENSE.md\n",
    "\n",
    "import os\n",
    "import math\n",
    "from define_models import baselines\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from critdd import Diagram, Diagrams\n",
    "from collections import defaultdict\n",
    "from numbers import Number\n",
    "import texfig as tf\n",
    "from texfig import TMLR_textwidth\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from benchmark_analysis import load_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dirs = [\n",
    "    (\n",
    "        \"weak/gb\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/estim/weak/gb\"),\n",
    "    ),\n",
    "    (\n",
    "        \"weak/klm\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/estim/weak/klm\"),\n",
    "    ),\n",
    "    (\n",
    "        \"noise/gb\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/estim/noise/gb\"),\n",
    "    ),\n",
    "    (\n",
    "        \"noise/klm\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/estim/noise/klm\"),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grid(axis):\n",
    "    axis.grid(c=\"#f2f2f2\", which=\"both\")\n",
    "    axis.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['allclass'] = dict()\n",
    "for prefix, result_dir in result_dirs:\n",
    "    all_results['allclass'][prefix] = load_estim(\n",
    "        result_dir=result_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"relabel\"] = dict()\n",
    "for prefix, result_dir in result_dirs:\n",
    "    result_dir = result_dir.replace(\"estim\", \"relabel\")\n",
    "    all_results[\"relabel\"][prefix] = load_estim(\n",
    "        result_dir=result_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['byclass'] = dict()\n",
    "for prefix, result_dir in result_dirs:\n",
    "    result_dir = result_dir.replace(\"estim\", \"estim_byclass\")\n",
    "    all_results['byclass'][prefix] = load_estim(\n",
    "        result_dir=result_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = np.unique(\n",
    "    np.concatenate([r.detector_name.unique() for r in all_results['allclass'].values()])\n",
    ")\n",
    "datasets = np.unique(np.concatenate([r.dataset_name.unique() for r in all_results['allclass'].values()]))\n",
    "\n",
    "detectors_nobaseline = list(set(detectors) - set(baselines + [\"random\"]))\n",
    "\n",
    "len(detectors), detectors, len(detectors_nobaseline), detectors_nobaseline, len(\n",
    "    datasets\n",
    "), datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_base_model_map = dict()\n",
    "d_detect_map = dict()\n",
    "\n",
    "for d in detectors_nobaseline:\n",
    "    s = d.split(\"_\")\n",
    "    if len(s) == 2:\n",
    "        d_base_model_map[d] = s[0]\n",
    "    else:\n",
    "        d_base_model_map[d] = \"klm\"\n",
    "\n",
    "    d_detect_map[d] = s[-1]\n",
    "\n",
    "detector_suffixes = np.unique(list(d_detect_map.values()))\n",
    "d_base_model_map, d_detect_map, detector_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_pretty_name = {\n",
    "    \"agra\": \"AGRA\",\n",
    "    \"aum\": \"AUM\",\n",
    "    \"cleanlab\": \"Cleanlab\",\n",
    "    \"consensus\": \"Consensus\",\n",
    "    \"forget\": \"Forget scores\",\n",
    "    \"influence\": \"Influence\",\n",
    "    \"representer\": \"Representer\",\n",
    "    \"smallloss\": \"Small loss\",\n",
    "    \"tracin\": \"TracIn\",\n",
    "    \"vosg\": \"VoSG\",\n",
    "}\n",
    "\n",
    "bm_pretty_name = {\"klm\": \"KLM\", \"gb\": \"GBT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps\n",
    "\n",
    "# tab_colors = colormaps[\"tab20\"]\n",
    "tab_colors = colormaps[\"tab10\"]\n",
    "\n",
    "detector_colors = {\n",
    "    d: tab_colors.colors[i % len(tab_colors.colors)]\n",
    "    for i, d in enumerate(detector_suffixes)\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "labels = []\n",
    "# KLM\n",
    "for d_name, d_color in detector_colors.items():\n",
    "    l = f\"({bm_pretty_name['klm']}) \" + detect_pretty_name[d_name]\n",
    "    plt.scatter([], [], label=l, color=d_color)\n",
    "    labels.append(l)\n",
    "\n",
    "# GB\n",
    "for d_name, d_color in detector_colors.items():\n",
    "    if \"gb_\" + d_name in detectors_nobaseline:\n",
    "        l = f\"({bm_pretty_name['gb']}) \" + detect_pretty_name[d_name]\n",
    "        plt.scatter([], [], label=l, color=d_color, marker=\"s\")\n",
    "        labels.append(l)\n",
    "\n",
    "    else:\n",
    "        plt.scatter([], [], label=\"\", color=\"white\", alpha=0)\n",
    "        labels.append(\"\")\n",
    "\n",
    "\n",
    "disp_baselines = False\n",
    "if disp_baselines:\n",
    "    plt.scatter([], [], label=\"gold\", color=\"black\", marker=\"x\")\n",
    "    labels.append(\"gold\")\n",
    "    plt.scatter([], [], label=\"silver\", color=\"black\", marker=\"+\")\n",
    "    labels.append(\"silver\")\n",
    "    plt.scatter([], [], label=\"random\", color=\"black\", marker=\"p\")\n",
    "    labels.append(\"random\")\n",
    "\n",
    "    for i in range(len(detector_colors.items()) - 3):\n",
    "        plt.scatter([], [], label=\"\", color=\"white\", alpha=0)\n",
    "        labels.append(\"\")\n",
    "\n",
    "plt.gca().axis(\"off\")\n",
    "plt.legend(ncols=3 if disp_baselines else 2, labels=labels, title=\"Detectors\")\n",
    "# plt.savefig(f\"figures/summary/{prefix_txt}_legend.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_detector_colors = dict()\n",
    "full_detector_markers = dict()\n",
    "for d in detectors:\n",
    "    if d in baselines + ['random']:\n",
    "        full_detector_colors[d] = 'white'\n",
    "        full_detector_markers[d] = 'x'\n",
    "    else:\n",
    "        full_detector_colors[d] = detector_colors[d_detect_map[d]]\n",
    "        full_detector_markers[d] = 'o' if d_base_model_map[d] == 'klm' else 's'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in all_results['allclass'].items():\n",
    "    d = r.pivot_table(\n",
    "        index=\"detector_name\",\n",
    "        columns=\"dataset_name\",\n",
    "        values=\"estim_time\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    print(k, d.sum().sum(), d.sum())\n",
    "    display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in all_results['relabel'].items():\n",
    "    d = r.pivot_table(\n",
    "        index=\"detector_name\",\n",
    "        columns=\"dataset_name\",\n",
    "        values=\"estim_time\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    print(k, d.sum().sum())\n",
    "\n",
    "    display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in all_results['byclass'].items():\n",
    "    d = r.pivot_table(\n",
    "        index=\"detector_name\",\n",
    "        columns=\"dataset_name\",\n",
    "        values=\"estim_time\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "\n",
    "    print(k, d.sum().sum())\n",
    "    display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in all_results['allclass'].items():\n",
    "    print(k)\n",
    "    display(\n",
    "        r.pivot_table(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"logl_test\",\n",
    "            aggfunc=\"min\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters selection\n",
    "\n",
    "all_results_cv = dict()\n",
    "all_val_cv = dict()\n",
    "\n",
    "baselines_norandom = set(baselines) - {\"random\"}\n",
    "for which, _all_res in [\n",
    "    (\"allclass\", all_results['allclass']),\n",
    "    (\"byclass\", all_results['byclass']),\n",
    "    (\"relabel\", all_results['relabel']),\n",
    "]:\n",
    "\n",
    "    all_results_cv[which] = dict()\n",
    "    all_val_cv[which] = dict()\n",
    "\n",
    "    for prefix in _all_res.keys():\n",
    "\n",
    "        all_results_cv[which][prefix] = dict()\n",
    "        all_val_cv[which][prefix] = dict()\n",
    "\n",
    "        for cv_k, cv_function in [\n",
    "            (\"logl\", lambda d, k: d[f\"logl_{k}\"].idxmin()),\n",
    "            (\"bacc\", lambda d, k: d[f\"bacc_{k}\"].idxmax()),\n",
    "        ]:\n",
    "\n",
    "            indices_oracl_cv = cv_function(\n",
    "                _all_res[prefix].groupby([\"dataset_name\", \"detector_name\"]), \"test\"\n",
    "            )\n",
    "            indices_clean_cv = cv_function(\n",
    "                _all_res[prefix].groupby([\"dataset_name\", \"detector_name\"]), \"val\"\n",
    "            )\n",
    "            indices_noisy_cv = cv_function(\n",
    "                _all_res[prefix].groupby([\"dataset_name\", \"detector_name\"]), \"noisy_val\"\n",
    "            )\n",
    "            indices_noisy_10_cv = cv_function(\n",
    "                _all_res[prefix][\n",
    "                    _all_res[prefix][\"params_splitter\"].apply(\n",
    "                        lambda r: \"quantile\" in r.keys() and r[\"quantile\"] == 0.1\n",
    "                    )\n",
    "                    | _all_res[prefix][\"detector_name\"].isin(baselines_norandom)\n",
    "                ].groupby([\"dataset_name\", \"detector_name\"]),\n",
    "                \"noisy_val\",\n",
    "            )\n",
    "            indices_noisy_90_cv = cv_function(\n",
    "                _all_res[prefix][\n",
    "                    _all_res[prefix][\"params_splitter\"].apply(\n",
    "                        lambda r: \"quantile\" in r.keys() and r[\"quantile\"] == 0.9\n",
    "                    )\n",
    "                    | _all_res[prefix][\"detector_name\"].isin(baselines_norandom)\n",
    "                ].groupby([\"dataset_name\", \"detector_name\"]),\n",
    "                \"noisy_val\",\n",
    "            )\n",
    "            indices_clean_90_cv = cv_function(\n",
    "                _all_res[prefix][\n",
    "                    _all_res[prefix][\"params_splitter\"].apply(\n",
    "                        lambda r: \"quantile\" in r.keys() and r[\"quantile\"] == 0.9\n",
    "                    )\n",
    "                    | _all_res[prefix][\"detector_name\"].isin(baselines_norandom)\n",
    "                ].groupby([\"dataset_name\", \"detector_name\"]),\n",
    "                \"val\",\n",
    "            )\n",
    "            indices_clean_10_cv = cv_function(\n",
    "                _all_res[prefix][\n",
    "                    _all_res[prefix][\"params_splitter\"].apply(\n",
    "                        lambda r: \"quantile\" in r.keys() and r[\"quantile\"] == 0.1\n",
    "                    )\n",
    "                    | _all_res[prefix][\"detector_name\"].isin(baselines_norandom)\n",
    "                ].groupby([\"dataset_name\", \"detector_name\"]),\n",
    "                \"val\",\n",
    "            )\n",
    "\n",
    "            all_results_cv[which][prefix][cv_k] = {\n",
    "                \"oracl\": _all_res[prefix].loc[indices_oracl_cv.dropna()],\n",
    "                \"clean\": _all_res[prefix].loc[indices_clean_cv.dropna()],\n",
    "                \"noisy\": _all_res[prefix].loc[indices_noisy_cv.dropna()],\n",
    "                \"noisy_10\": _all_res[prefix].loc[indices_noisy_10_cv.dropna()],\n",
    "                \"noisy_90\": _all_res[prefix].loc[indices_noisy_90_cv.dropna()],\n",
    "                \"clean_90\": _all_res[prefix].loc[indices_clean_90_cv.dropna()],\n",
    "                \"clean_10\": _all_res[prefix].loc[indices_clean_10_cv.dropna()],\n",
    "            }\n",
    "\n",
    "            all_val_cv[which][prefix][cv_k] = {\n",
    "                k: r.pivot(\n",
    "                    index=\"detector_name\", columns=\"dataset_name\", values=f\"{cv_k}_test\"\n",
    "                )\n",
    "                for k, r in all_results_cv[which][prefix][cv_k].items()\n",
    "            }\n",
    "\n",
    "        # sanity check\n",
    "        best_logl_direct = _all_res[prefix].pivot_table(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"logl_test\",\n",
    "            aggfunc=\"min\",\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            best_logl_direct - all_val_cv[which][prefix][\"logl\"][\"oracl\"]\n",
    "        ).abs().sum().sum() == 0\n",
    "\n",
    "        best_bacc_direct = _all_res[prefix].pivot_table(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"bacc_test\",\n",
    "            aggfunc=\"max\",\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            best_bacc_direct - all_val_cv[which][prefix][\"bacc\"][\"oracl\"]\n",
    "        ).abs().sum().sum() == 0\n",
    "\n",
    "hp_tune_str = {\n",
    "    \"oracl\": \"oracle\",\n",
    "    \"clean\": \"clean validation set\",\n",
    "    \"noisy\": \"noisy validation set\",\n",
    "    \"noisy_10\": \"noisy validation set (threshold=10%)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = all_results_cv[\"allclass\"][\"weak/gb\"][\"bacc\"][\"clean\"]\n",
    "\n",
    "e = d[np.logical_and(d[\"detector_name\"] == \"none\", d[\"dataset_name\"] == \"agnews\")]\n",
    "\n",
    "print(e[\"params_classifier\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "\n",
    "results_cv = all_results_cv[exp_alt][prefix][\"logl\"]\n",
    "logl_cv = all_val_cv[exp_alt][prefix][\"logl\"]\n",
    "logl_cv_for_norm = all_val_cv[\"allclass\"][prefix][\"logl\"]\n",
    "\n",
    "roc_auc = results_cv[\"oracl\"].pivot(\n",
    "    index=\"detector_name\", columns=\"dataset_name\", values=\"global_ranking_quality\"\n",
    ")\n",
    "\n",
    "n_per_row = 4\n",
    "n_rows = math.ceil(len(datasets) / n_per_row)\n",
    "fig, axes = plt.subplots(n_rows, n_per_row, figsize=(13, 3 * n_rows))\n",
    "\n",
    "for i, dataset_name in enumerate(results_cv[\"oracl\"].dataset_name.unique()):\n",
    "    axis = axes[i // n_per_row, i % n_per_row]\n",
    "\n",
    "    axis.set_title(dataset_name)\n",
    "\n",
    "    d_colors = [full_detector_colors[d] for d in roc_auc.index]\n",
    "\n",
    "    axis.scatter(\n",
    "        roc_auc[dataset_name],\n",
    "        logl_cv[\"oracl\"][dataset_name],\n",
    "        marker=\"+\",\n",
    "        c=d_colors,\n",
    "    )\n",
    "\n",
    "    perf_none = logl_cv_for_norm[\"oracl\"][dataset_name][\"none\"]\n",
    "    perf_silver = logl_cv_for_norm[\"oracl\"][dataset_name][\"silver\"]\n",
    "    axis.hlines(perf_none, 0.35, 1)\n",
    "    axis.hlines(perf_silver, 0.35, 1)\n",
    "\n",
    "    # axis.set_xlim(0.35, 1)\n",
    "    axis.set_ylim(2 * perf_silver - perf_none, 2 * perf_none - perf_silver)\n",
    "    custom_grid(axis)\n",
    "\n",
    "for axis in axes:\n",
    "    axis[0].set_ylabel(\"test log loss\")\n",
    "for axis in axes[-1]:\n",
    "    axis.set_xlabel(\"roc auc\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "results_cv = all_results_cv[exp_alt][prefix][\"bacc\"]\n",
    "logl_cv = all_val_cv[exp_alt][prefix][\"bacc\"]\n",
    "logl_cv_for_norm = all_val_cv[\"allclass\"][prefix][\"bacc\"]\n",
    "\n",
    "n_per_row = 4\n",
    "n_rows = math.ceil(len(datasets) / n_per_row)\n",
    "fig, axes = plt.subplots(n_rows, n_per_row, figsize=(13, 3 * n_rows))\n",
    "\n",
    "for i, dataset_name in enumerate(results_cv[\"oracl\"].dataset_name.unique()):\n",
    "    axis = axes[i // n_per_row, i % n_per_row]\n",
    "\n",
    "    axis.set_title(dataset_name)\n",
    "\n",
    "    sorted_indices = np.argsort(np.argsort(logl_cv[\"oracl\"][dataset_name].values))\n",
    "    n_detectors = len(sorted_indices)\n",
    "\n",
    "    d_colors = [full_detector_colors[d] for d in logl_cv[\"oracl\"][dataset_name].index]\n",
    "\n",
    "    axis.scatter(\n",
    "        sorted_indices,\n",
    "        logl_cv[\"oracl\"][dataset_name],\n",
    "        marker=\"_\",\n",
    "        c=d_colors,\n",
    "    )\n",
    "\n",
    "    axis.scatter(\n",
    "        sorted_indices,\n",
    "        logl_cv[\"clean\"][dataset_name],\n",
    "        marker=\"x\",\n",
    "        c=d_colors,\n",
    "    )\n",
    "\n",
    "    axis.scatter(\n",
    "        sorted_indices,\n",
    "        logl_cv[\"noisy\"][dataset_name],\n",
    "        marker=\"*\",\n",
    "        c=d_colors,\n",
    "    )\n",
    "\n",
    "    perf_none = logl_cv_for_norm[\"oracl\"][dataset_name][\"none\"]\n",
    "    perf_silver = logl_cv_for_norm[\"oracl\"][dataset_name][\"silver\"]\n",
    "    axis.hlines(perf_none, -1, n_detectors)\n",
    "    axis.hlines(perf_silver, -1, n_detectors)\n",
    "\n",
    "    axis.set_ylim(2 * perf_silver - perf_none, 2 * perf_none - perf_silver)\n",
    "    axis.set_xlim(-1, n_detectors)\n",
    "    custom_grid(axis)\n",
    "\n",
    "for axis in axes:\n",
    "    axis[0].set_ylabel(\"test log loss\")\n",
    "\n",
    "plt.savefig(f\"figures/summary/{prefix.replace('/','_')}.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tune_str = {\n",
    "    \"clean\": \"HP tuned on noise free validation\",\n",
    "    \"noisy\": \"HP tuned on noisy validation\",\n",
    "    \"oracl\": \"Oracle HP\",\n",
    "    \"noisy_10\": \"HP tuned on noisy validation, threshold forced at 10\\%\",\n",
    "    \"noisy_90\": \"HP tuned on noisy validation, training on 10\\% top trusted\",\n",
    "    \"clean_90\": \"HP tuned on clean validation, training on 10\\% top trusted\",\n",
    "    \"clean_10\": \"HP tuned on clean validation, training on 90\\% top trusted\",\n",
    "}\n",
    "\n",
    "strategy_str = {\n",
    "    \"allclass\": \"Filtering\",\n",
    "    \"byclass\": \"Filtering by class\",\n",
    "    \"relabel\": \"10\\% relabeling\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "metric = \"logl\"\n",
    "\n",
    "results_cv = all_results_cv[exp_alt][prefix][metric]\n",
    "logl_cv = all_val_cv[exp_alt][prefix][metric]\n",
    "logl_cv_for_norm = all_val_cv[\"allclass\"][prefix][metric]\n",
    "\n",
    "for k, r in results_cv.items():\n",
    "    if exp_alt == \"relabel\" and k in [\"noisy_10\", \"noisy_90\", \"clean_10\", \"clean_90\"]:\n",
    "        continue\n",
    "\n",
    "    logl_cv_none = logl_cv_for_norm[k].loc[logl_cv_for_norm[k].index == \"none\"].values\n",
    "    logl_cv_random = logl_cv[k].loc[logl_cv[k].index == \"random\"].values\n",
    "    logl_cv_wood = logl_cv_for_norm[k].loc[logl_cv_for_norm[k].index == \"wood\"].values\n",
    "    logl_cv_silver = (\n",
    "        logl_cv_for_norm[k].loc[logl_cv_for_norm[k].index == \"silver\"].values\n",
    "    )\n",
    "    logl_cv_gold = logl_cv_for_norm[k].loc[logl_cv_for_norm[k].index == \"gold\"].values\n",
    "\n",
    "    # norm_low = (logl_cv_none + logl_cv_wood) / 2\n",
    "    # norm_high = (logl_cv_silver + logl_cv_gold) / 2\n",
    "\n",
    "    if False and exp_alt == \"relabel\":\n",
    "        norm_low = logl_cv_random\n",
    "    else:\n",
    "        norm_low = logl_cv_none\n",
    "    norm_high = logl_cv_silver\n",
    "\n",
    "    print(k, norm_low.shape, norm_high.shape, logl_cv[k].shape)\n",
    "    logl_cv_norm = (logl_cv[k] - norm_low) / (norm_high - norm_low + 1e-10)\n",
    "\n",
    "    ordered_detectors = list(logl_cv_norm.median(axis=1).sort_values().index)\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * 0.95, pad=0.5)\n",
    "\n",
    "    plt.axhline(100)\n",
    "    plt.axhline(200)\n",
    "\n",
    "    for i, detector_name in enumerate(ordered_detectors):\n",
    "        d = logl_cv_norm.loc[logl_cv_norm.index == detector_name].values[0]\n",
    "        # print(len(d))\n",
    "\n",
    "        d_base100 = (2 - d) * 100\n",
    "        d_base100 = d_base100[~np.isnan(d_base100)]\n",
    "        bplot = plt.boxplot(\n",
    "            [d_base100],\n",
    "            positions=[i],\n",
    "            # notch=True,\n",
    "            # bootstrap=100,\n",
    "            widths=[0.7],\n",
    "            showfliers=False,\n",
    "            patch_artist=True,\n",
    "        )\n",
    "\n",
    "        if np.any(np.isnan(d_base100)):\n",
    "            ccc\n",
    "\n",
    "        bplot[\"boxes\"][0].set_facecolor(full_detector_colors[detector_name])\n",
    "        bplot[\"boxes\"][0].set_alpha(0.8)\n",
    "\n",
    "        eps = 0.05\n",
    "        plt.scatter(\n",
    "            [i] * len(d_base100) + np.random.uniform(-eps, eps, size=len(d_base100)),\n",
    "            d_base100,\n",
    "            facecolors=[full_detector_colors[detector_name]] * len(d_base100),\n",
    "            edgecolors=\"black\",\n",
    "            s=[5] * len(d_base100),\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    pretty_xticks = []\n",
    "    for s in ordered_detectors:\n",
    "        if s in baselines + [\"random\"]:\n",
    "            pretty_xticks.append(s.replace(\"_\", \"\\\\_\"))\n",
    "        else:\n",
    "            pretty_xticks.append(\n",
    "                detect_pretty_name[d_detect_map[s]]\n",
    "                + f\" ({bm_pretty_name[d_base_model_map[s]]})\"\n",
    "            )\n",
    "    plt.xticks(\n",
    "        range(len(ordered_detectors)),\n",
    "        pretty_xticks,\n",
    "        rotation=50,\n",
    "        rotation_mode=\"anchor\",\n",
    "        ha=\"right\",\n",
    "    )\n",
    "\n",
    "    for i, detector_name in enumerate(ordered_detectors):\n",
    "        if detector_name in baselines:\n",
    "            plt.gca().get_xticklabels()[i].set_color(\"red\")\n",
    "        elif detector_name in [\"random\"]:\n",
    "            plt.gca().get_xticklabels()[i].set_color(\"blue\")\n",
    "\n",
    "    plt.ylim(0, 300)\n",
    "    custom_grid(plt.gca())\n",
    "    # plt.title(f\"Hyperparameters tuned using {hp_tune_str[k]} | {prefix} | {exp_alt}\")\n",
    "    plt.ylabel(\"normalized test loss\")\n",
    "\n",
    "    noise_str = \"NCAR\" if prefix.split(\"/\")[0] == \"noise\" else \"NNAR\"\n",
    "    classif_str = (\n",
    "        \"Linear Classifier\"\n",
    "        if prefix.split(\"/\")[1] == \"klm\"\n",
    "        else \"Gradient Boosting Classifier\"\n",
    "    )\n",
    "    plt.suptitle(\n",
    "        f\"{noise_str} | {strategy_str[exp_alt]} | {classif_str} | {hp_tune_str[k]}\"\n",
    "    )\n",
    "\n",
    "    tf.savefig(f\"figures/detectors/{prefix.replace('/','_')}_{exp_alt}_{k}_{metric}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gb_klm_mix(prefix, exp_alt, method_suffixes, k=\"oracl\"):\n",
    "    classif_c = ((\"klm\", \"tab:blue\"), (\"gb\", \"tab:orange\"))\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    xy_min = 1000\n",
    "    xy_max = 0\n",
    "\n",
    "    # for method_suffix in [\"vosg\", \"consensus\", \"forget\", \"cleanlab\", \"aum\"]:\n",
    "    for method_suffix in method_suffixes:\n",
    "        for classif, color in classif_c:\n",
    "            logl_cv_classif = all_val_cv[exp_alt][prefix + classif][\"logl\"][k]\n",
    "\n",
    "            perf_klm = logl_cv_classif.loc[\n",
    "                logl_cv_classif.index == \"klm_\" + method_suffix\n",
    "            ].values\n",
    "            perf_gb = logl_cv_classif.loc[\n",
    "                logl_cv_classif.index == \"gb_\" + method_suffix\n",
    "            ].values\n",
    "\n",
    "            # print(perf_gb.shape, perf_klm.shape)\n",
    "            # print(perf_gb[:2], perf_klm[:2])\n",
    "            is_na = np.logical_or(np.isnan(perf_gb), np.isnan(perf_klm))\n",
    "\n",
    "            plt.scatter(perf_gb[~is_na], perf_klm[~is_na], color=color, s=12, alpha=0.7)\n",
    "\n",
    "            xy_min = min([xy_min, np.nanmin(perf_gb), np.nanmin(perf_klm)])\n",
    "            xy_max = max([xy_max, np.nanmax(perf_gb), np.nanmax(perf_klm)])\n",
    "\n",
    "    xlims, ylims = plt.xlim(), plt.ylim()\n",
    "    # xy_min = min(xlims[0], ylims[0])\n",
    "    # xy_max = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([xy_min, xy_max], [xy_min, xy_max], color=\"black\")\n",
    "    plt.ylabel(\"test log loss\\n(KLM base model)\")\n",
    "    plt.xlabel(\"test log loss\\n(GBM base model)\")\n",
    "\n",
    "    for base_model, color in classif_c:\n",
    "        plt.scatter(\n",
    "            [],\n",
    "            [],\n",
    "            color=color,\n",
    "            label=f\"{bm_pretty_name[base_model]} estimator\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "    plt.xlim(1e-1, 1)\n",
    "    plt.ylim(1e-1, 1)\n",
    "    # plt.xlim(xy_min, xy_max)\n",
    "    # plt.ylim(xy_min, xy_max)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.xticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.yticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.xticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    plt.yticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    # plt.grid()\n",
    "\n",
    "    tf.savefig(f\"figures/robustness/{prefix.replace('/','_')}_{exp_alt}\")\n",
    "    plt.show()\n",
    "\n",
    "for exp_alt in ['allclass', 'byclass', 'relabel']:\n",
    "    for prefix in ['weak/', 'noise/']:\n",
    "        print(exp_alt, prefix)\n",
    "        plot_gb_klm_mix(\n",
    "            prefix=prefix,\n",
    "            exp_alt=exp_alt,\n",
    "            method_suffixes=[\"smallloss\", \"consensus\", \"cleanlab\", \"aum\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_byclass_vs_allclass(prefix, k):\n",
    "    logl_cv_allclass = all_val_cv[\"allclass\"][prefix][\"logl\"][k]\n",
    "    logl_cv_byclass = all_val_cv[\"byclass\"][prefix][\"logl\"][k]\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    for detector in set(detectors) - set(baselines):\n",
    "\n",
    "        perf_allclass = logl_cv_allclass.loc[logl_cv_allclass.index == detector]\n",
    "        perf_byclass = logl_cv_byclass.loc[logl_cv_byclass.index == detector]\n",
    "\n",
    "        # print(detector, perf_allclass.shape, perf_byclass.shape)\n",
    "\n",
    "        plt.scatter(\n",
    "            perf_allclass,\n",
    "            perf_byclass,\n",
    "            color=full_detector_colors[detector],\n",
    "            marker=full_detector_markers[detector],\n",
    "            s=12,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    xlims, ylims = plt.xlim(), plt.ylim()\n",
    "    xy_min = min(xlims[0], ylims[0])\n",
    "    xy_max = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([xy_min, xy_max], [xy_min, xy_max], color=\"black\")\n",
    "    plt.xlabel(\"test log loss\\n(filtering allclass)\")\n",
    "    plt.ylabel(\"test log loss\\n(filtering byclass)\")\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "    plt.xlim(1e-1, 1)\n",
    "    plt.ylim(1e-1, 1)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.xticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.yticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.xticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    plt.yticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    # plt.grid()\n",
    "\n",
    "    tf.savefig(f\"figures/byclass_vs_allclass/{prefix.replace('/','_')}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_byclass_vs_allclass(prefix=\"weak/klm\", k=\"oracl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clean10_vs_clean90(prefix, exp_alt, k=\"clean\"):\n",
    "    logl_cv_for_norm = all_val_cv[exp_alt][prefix][\"logl\"]\n",
    "\n",
    "    norm_low = logl_cv_for_norm[k].loc[logl_cv_for_norm[k].index == \"none\"].values\n",
    "    norm_high = logl_cv_for_norm[k].loc[logl_cv_for_norm[k].index == \"silver\"].values\n",
    "\n",
    "    logl_cv_clean10 = all_val_cv[exp_alt][prefix][\"logl\"][\"clean_10\"]\n",
    "    logl_cv_clean90 = all_val_cv[exp_alt][prefix][\"logl\"][\"clean_90\"]\n",
    "\n",
    "    logl_cv_clean10 = (logl_cv_clean10 - norm_low) / (norm_high - norm_low + 1e-10)\n",
    "    logl_cv_clean90 = (logl_cv_clean90 - norm_low) / (norm_high - norm_low + 1e-10)\n",
    "\n",
    "    logl_cv_clean10 = (logl_cv_clean10 + 1) * 100\n",
    "    logl_cv_clean90 = (logl_cv_clean90 + 1) * 100\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    for detector in set(detectors) - set(baselines):\n",
    "\n",
    "        perf_clean10 = logl_cv_clean10.loc[logl_cv_clean10.index == detector]\n",
    "        perf_clean90 = logl_cv_clean90.loc[logl_cv_clean90.index == detector]\n",
    "\n",
    "        plt.scatter(\n",
    "            perf_clean10,\n",
    "            perf_clean90,\n",
    "            color=full_detector_colors[detector],\n",
    "            marker=full_detector_markers[detector],\n",
    "            s=12,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    xlims, ylims = plt.xlim(), plt.ylim()\n",
    "    xy_min = min(xlims[0], ylims[0])\n",
    "    xy_max = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([xy_min, xy_max], [xy_min, xy_max], color=\"black\")\n",
    "    plt.xlabel(\"test log loss\\n(training on 10\\% most trusted)\")\n",
    "    plt.ylabel(\"test log loss\\n(training without 10\\% less trusted)\")\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "    plt.xlim(0, 250)\n",
    "    plt.ylim(0, 250)\n",
    "    # plt.xscale(\"log\")\n",
    "    # plt.yscale(\"log\")\n",
    "\n",
    "    plt.xticks(np.arange(0, 250, 50), [], minor=True)\n",
    "    plt.yticks(np.arange(0, 250, 50), [], minor=True)\n",
    "    plt.xticks([0, 100, 200], [0, 100, 200], minor=False)\n",
    "    plt.yticks([0, 100, 200], [0, 100, 200], minor=False)\n",
    "    # plt.grid()\n",
    "\n",
    "    tf.savefig(f\"figures/top_vs_bottom/{prefix.replace('/','_')}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_clean10_vs_clean90(prefix=\"weak/klm\", exp_alt=\"allclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nnar_vs_ncar(suffix, k, exp_alt):\n",
    "    logl_cv_noise = all_val_cv[exp_alt][\"noise\" + suffix][\"logl\"][k]\n",
    "    logl_cv_weak = all_val_cv[exp_alt][\"weak\" + suffix][\"logl\"][k]\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    for detector in set(detectors) - set(baselines):\n",
    "\n",
    "        perf_noise = logl_cv_noise.loc[logl_cv_noise.index == detector]\n",
    "        perf_weak = logl_cv_weak.loc[logl_cv_weak.index == detector]\n",
    "\n",
    "        if perf_noise.shape != perf_weak.shape:\n",
    "            continue\n",
    "\n",
    "        plt.scatter(\n",
    "            perf_noise,\n",
    "            perf_weak,\n",
    "            color=full_detector_colors[detector],\n",
    "            s=12,\n",
    "            alpha=0.7,\n",
    "            marker=full_detector_markers[detector],\n",
    "        )\n",
    "\n",
    "    xlims, ylims = plt.xlim(), plt.ylim()\n",
    "    xy_min = min(xlims[0], ylims[0])\n",
    "    xy_max = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([xy_min, xy_max], [xy_min, xy_max], color=\"black\")\n",
    "    plt.xlabel(\"test log loss\\n(NCAR dataset)\")\n",
    "    plt.ylabel(\"test log loss\\n(NNAR dataset)\")\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "    plt.xlim(1e-1, 1)\n",
    "    plt.ylim(1e-1, 1)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.xticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.yticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.xticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    plt.yticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    # plt.grid()\n",
    "\n",
    "    tf.savefig(f\"figures/weak_vs_noise/{exp_alt}{suffix.replace('/','_')}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_nnar_vs_ncar(suffix=\"/klm\", k=\"oracl\", exp_alt=\"allclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clean_noisy(prefix, exp_alt):\n",
    "    logl_cv_noisy = all_val_cv[exp_alt][prefix][\"logl\"][\"noisy\"]\n",
    "    logl_cv_clean = all_val_cv[exp_alt][prefix][\"logl\"][\"clean\"]\n",
    "    logl_cv_noisy_none = logl_cv_noisy.loc[logl_cv_noisy.index == \"none\"]\n",
    "    logl_cv_clean_none = logl_cv_clean.loc[logl_cv_clean.index == \"none\"]\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    for detector in set(detectors) - set(baselines):\n",
    "\n",
    "        perf_noisy = logl_cv_noisy.loc[logl_cv_noisy.index == detector]\n",
    "        perf_clean = logl_cv_clean.loc[logl_cv_clean.index == detector]\n",
    "\n",
    "        plt.scatter(\n",
    "            logl_cv_noisy_none,\n",
    "            perf_noisy,\n",
    "            color=full_detector_colors[detector],\n",
    "            s=12,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        plt.scatter(\n",
    "            logl_cv_clean_none,\n",
    "            perf_clean,\n",
    "            color=full_detector_colors[detector],\n",
    "            s=12,\n",
    "            alpha=0.7,\n",
    "            marker=\"x\",\n",
    "        )\n",
    "\n",
    "    xlims, ylims = plt.xlim(), plt.ylim()\n",
    "    xy_min = min(xlims[0], ylims[0])\n",
    "    xy_max = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([xy_min, xy_max], [xy_min, xy_max], color=\"black\")\n",
    "\n",
    "    plt.xlabel(\"test log loss\\n(no filtering)\")\n",
    "    plt.ylabel(\"test log loss\\n(detect + filter)\")\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "    plt.xlim(1e-1, 1)\n",
    "    plt.ylim(1e-1, 1)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.xticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.yticks(np.arange(0.1, 1, 0.1), [], minor=True)\n",
    "    plt.xticks([0.1, 1], [0.1, 1], minor=False)\n",
    "    plt.yticks([0.1, 1], [0.1, 1], minor=False)\n",
    "\n",
    "    plt.scatter([], [], color=\"black\", label=f\"clean valid.\", marker=\"x\", s=12)\n",
    "    plt.scatter([], [], color=\"black\", label=f\"noisy valid.\", s=12)\n",
    "    plt.legend()\n",
    "\n",
    "    tf.savefig(f\"figures/clean_vs_noisy/{exp_alt}_{prefix.replace('/','_')}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_clean_noisy(prefix=\"weak/klm\", exp_alt=\"allclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"noise/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "logl_cv_noisy = all_val_cv[exp_alt][prefix][\"logl\"][\"noisy\"]\n",
    "logl_cv_clean = all_val_cv[exp_alt][prefix][\"logl\"][\"clean\"]\n",
    "logl_cv_noisy_none = logl_cv_noisy.loc[logl_cv_noisy.index == \"none\"]\n",
    "logl_cv_clean_none = logl_cv_clean.loc[logl_cv_clean.index == \"none\"]\n",
    "\n",
    "perf_noisy = logl_cv_noisy.loc[logl_cv_noisy.index == \"gb_smallloss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_quantiles = []\n",
    "noise_ratios = []\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "for p_split, noise_ratio in all_results_cv[\"allclass\"][\"noise/klm\"][\"logl\"][\"oracl\"][\n",
    "    [\"params_splitter\", \"noise_ratio\"]\n",
    "].itertuples(index=False):\n",
    "    if \"quantile\" in p_split.keys():\n",
    "        splitter_quantiles.append(p_split[\"quantile\"])\n",
    "        noise_ratios.append(noise_ratio)\n",
    "\n",
    "plt.scatter(noise_ratios, splitter_quantiles, alpha=0.2)\n",
    "plt.grid()\n",
    "plt.xlabel(\"noise ratio\")\n",
    "plt.ylabel(\"splitter quantile\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_scales = {\n",
    "    \"alpha\": \"log\",\n",
    "    \"eta0\": \"log\",\n",
    "    \"learning_rate\": \"log\",\n",
    "    \"reg_lambda\": \"linear\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_alt = \"allclass\"\n",
    "prefix = \"weak/klm\"\n",
    "\n",
    "vals = defaultdict(list)\n",
    "cv_k = \"logl\"\n",
    "\n",
    "for params_detector in all_results_cv[exp_alt][prefix][cv_k][\"oracl\"][\n",
    "    \"params_detector\"\n",
    "]:\n",
    "    if isinstance(params_detector, dict):\n",
    "        for k, v in params_detector.items():\n",
    "            if type(v) == float:\n",
    "                vals[k].append(v)\n",
    "\n",
    "for params_classifier in all_results_cv[exp_alt][prefix][cv_k][\"oracl\"][\n",
    "    \"params_classifier\"\n",
    "]:\n",
    "    if isinstance(params_classifier, dict):\n",
    "        for k, v in params_classifier.items():\n",
    "            if type(v) == float:\n",
    "                vals[k].append(v)\n",
    "\n",
    "f, axes = plt.subplots(1, len(vals), figsize=(3 * len(vals), 3))\n",
    "for i, (k, v) in enumerate(vals.items()):\n",
    "\n",
    "    scale = hp_scales[k.split(\"__\")[-1]]\n",
    "    axis = axes[i]\n",
    "\n",
    "    if scale == \"log\":\n",
    "        axis.hist(np.log10(v))\n",
    "    else:\n",
    "        axis.hist(v)\n",
    "    axis.set_title(k)\n",
    "\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_alt = \"allclass\"\n",
    "prefix = \"weak/klm\"\n",
    "\n",
    "vals = defaultdict(lambda: defaultdict(list))\n",
    "cv_k = \"logl\"\n",
    "\n",
    "for params_detector, detect_name in all_results_cv[exp_alt][prefix][cv_k][\"oracl\"][\n",
    "    [\"params_detector\", \"detector_name\"]\n",
    "].itertuples(index=False):\n",
    "    if isinstance(params_detector, dict):\n",
    "        for k, v in params_detector.items():\n",
    "            if type(v) == float:\n",
    "                vals[detect_name][k].append(v)\n",
    "\n",
    "for params_classifier, detect_name in all_results_cv[exp_alt][prefix][cv_k][\"oracl\"][\n",
    "    [\"params_classifier\", \"detector_name\"]\n",
    "].itertuples(index=False):\n",
    "    if isinstance(params_classifier, dict):\n",
    "        for k, v in params_classifier.items():\n",
    "            if type(v) == float:\n",
    "                vals[detect_name][k].append(v)\n",
    "\n",
    "n_hps = np.max([len(v) for v in vals.values()])\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(len(vals), n_hps, figsize=(3 * n_hps, 3 * len(vals)))\n",
    "\n",
    "for j, (k_detect, vals_d) in enumerate(vals.items()):\n",
    "    axes[j][0].set_ylabel(k_detect)\n",
    "\n",
    "    for i, (k, v) in enumerate(vals_d.items()):\n",
    "\n",
    "        scale = hp_scales[k.split(\"__\")[-1]]\n",
    "        axis = axes[j][i]\n",
    "\n",
    "        if scale == \"log\":\n",
    "            axis.hist(np.log10(v))\n",
    "        else:\n",
    "            axis.hist(v)\n",
    "\n",
    "        axis.set_title(k)\n",
    "\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_alt = \"allclass\"\n",
    "prefix = \"noise/klm\"\n",
    "\n",
    "r_oracl = all_results_cv[exp_alt][prefix][\"logl\"][\"oracl\"]\n",
    "r_noisy = all_results_cv[exp_alt][prefix][\"logl\"][\"noisy\"]\n",
    "\n",
    "# p = r_noisy.pivot(index=\"dataset_name\", columns=\"detector_name\")\n",
    "tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "custom_grid(plt.gca())\n",
    "\n",
    "for detect_name, dataset_name, p_split_oracl in r_oracl[\n",
    "    [\"detector_name\", \"dataset_name\", \"params_splitter\"]\n",
    "].itertuples(index=False):\n",
    "    p_split_noisy = r_noisy[\n",
    "        (r_noisy[\"dataset_name\"] == dataset_name)\n",
    "        & (r_noisy[\"detector_name\"] == detect_name)\n",
    "    ][\"params_splitter\"].iloc[0]\n",
    "\n",
    "    if detect_name in [\"agra\", \"random\"] or not \"quantile\" in p_split_oracl:\n",
    "        continue\n",
    "\n",
    "    plt.scatter(\n",
    "        p_split_oracl[\"quantile\"],\n",
    "        p_split_noisy[\"quantile\"],\n",
    "        alpha=0.15,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "\n",
    "lmin = -0.05\n",
    "lmax = 1.05\n",
    "\n",
    "plt.xlim(lmin, lmax)\n",
    "plt.ylim(lmin, lmax)\n",
    "plt.plot([lmin, lmax], [lmin, lmax], color=\"black\")\n",
    "plt.xlabel(\"Filtering quantile\\nOracle\")\n",
    "plt.ylabel(\"Filtering quantile\\nTuned using noisy valid. set\")\n",
    "plt.xticks(np.linspace(0, 1, 6))\n",
    "plt.yticks(np.linspace(0, 1, 6))\n",
    "\n",
    "tf.savefig(f\"figures/threshold/{prefix.replace('/','_')}_{exp_alt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "\n",
    "def scale_f_mult(x, up=True):\n",
    "    if up:\n",
    "        return x * 1.5\n",
    "    return x / 1.5\n",
    "\n",
    "\n",
    "def scale_f_add(x, up=True):\n",
    "    if up:\n",
    "        return x + 5\n",
    "    return x - 5\n",
    "\n",
    "\n",
    "for k_detect, k_classif, scale, scale_f in [\n",
    "    (\"base_model__sgd__alpha\", \"sgd__alpha\", \"log\", scale_f_mult),\n",
    "    (\"base_model__sgd__eta0\", \"sgd__eta0\", \"log\", scale_f_mult),\n",
    "    (\"base_model__reg_lambda\", \"reg_lambda\", \"linear\", scale_f_add),\n",
    "]:\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    d_names = []\n",
    "    counts = {\"above\": 0, \"below\": 0, \"equal\": 0}\n",
    "\n",
    "    for detect_name, p_detect, p_classif in all_results_cv[exp_alt][prefix][\"logl\"][\n",
    "        \"oracl\"\n",
    "    ][[\"detector_name\", \"params_detector\", \"params_classifier\"]].itertuples(\n",
    "        index=False\n",
    "    ):\n",
    "\n",
    "        if (\n",
    "            isinstance(p_detect, dict)\n",
    "            and k_detect in p_detect.keys()\n",
    "            and k_classif in p_classif.keys()\n",
    "        ):\n",
    "            p1.append(p_detect[k_detect])\n",
    "            p2.append(p_classif[k_classif])\n",
    "            d_names.append(detect_name)\n",
    "\n",
    "            if p_classif[k_classif] > scale_f(p_detect[k_detect], True):\n",
    "                counts[\"above\"] += 1\n",
    "            elif p_classif[k_classif] < scale_f(p_detect[k_detect], False):\n",
    "                counts[\"below\"] += 1\n",
    "            else:\n",
    "                counts[\"equal\"] += 1\n",
    "\n",
    "    if len(p1) == 0:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(p1, p2, c=[full_detector_colors[d] for d in d_names], alpha=1)\n",
    "    plt.grid()\n",
    "    plt.xlabel(k_detect)\n",
    "    plt.ylabel(k_classif)\n",
    "    plt.xscale(scale)\n",
    "    plt.yscale(scale)\n",
    "\n",
    "    xlims = plt.xlim()\n",
    "    ylims = plt.ylim()\n",
    "\n",
    "    rmin = min(xlims[0], ylims[0])\n",
    "    rmax = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([rmin, rmax], [rmin, rmax])\n",
    "\n",
    "    plt.plot([scale_f(rmin, True), rmax], [rmin, scale_f(rmax, False)], color=\"orange\")\n",
    "    plt.plot([rmin, scale_f(rmax, False)], [scale_f(rmin, True), rmax], color=\"orange\")\n",
    "\n",
    "    plt.xlim(rmin, rmax)\n",
    "    plt.ylim(rmin, rmax)\n",
    "\n",
    "    plt.title(f\"{counts['above']} / {counts['equal']} / {counts['below']}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "\n",
    "def scale_f_mult(x, up=True):\n",
    "    if up:\n",
    "        return x * 1.5\n",
    "    return x / 1.5\n",
    "\n",
    "\n",
    "def scale_f_add(x, up=True):\n",
    "    if up:\n",
    "        return x + 5\n",
    "    return x - 5\n",
    "\n",
    "\n",
    "r_none = all_results_cv[exp_alt][prefix][\"logl\"][\"oracl\"][\n",
    "    all_results_cv[exp_alt][prefix][\"logl\"][\"oracl\"][\"detector_name\"] == \"none\"\n",
    "]\n",
    "\n",
    "for k_classif, scale, scale_f in [\n",
    "    (\"sgd__alpha\", \"log\", scale_f_mult),\n",
    "    (\"sgd__eta0\", \"log\", scale_f_mult),\n",
    "    (\"learning_rate\", \"log\", scale_f_mult),\n",
    "    (\"reg_lambda\", \"linear\", scale_f_add),\n",
    "]:\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    d_names = []\n",
    "    counts = {\"above\": 0, \"below\": 0, \"equal\": 0}\n",
    "\n",
    "    for detect_name, dataset_name, p_classif_filter, logl_filter in all_results_cv[\n",
    "        exp_alt\n",
    "    ][prefix][\"logl\"][\"oracl\"][\n",
    "        [\"detector_name\", \"dataset_name\", \"params_classifier\", \"logl_test\"]\n",
    "    ].itertuples(\n",
    "        index=False\n",
    "    ):\n",
    "\n",
    "        if detect_name in baselines + [\"random\"]:\n",
    "            continue\n",
    "\n",
    "        assert len(r_none[r_none[\"dataset_name\"] == dataset_name]) == 1\n",
    "        p_classif_none, logl_none = r_none[r_none[\"dataset_name\"] == dataset_name].iloc[\n",
    "            0\n",
    "        ][[\"params_classifier\", \"logl_test\"]]\n",
    "\n",
    "        if isinstance(p_classif_filter, dict) and k_classif in p_classif_filter.keys():\n",
    "            p2.append(p_classif_filter[k_classif])\n",
    "            p1.append(p_classif_none[k_classif])\n",
    "            d_names.append(detect_name)\n",
    "\n",
    "            if p_classif_filter[k_classif] > scale_f(p_classif_none[k_classif], True):\n",
    "                counts[\"above\"] += 1\n",
    "            elif p_classif_filter[k_classif] < scale_f(\n",
    "                p_classif_none[k_classif], False\n",
    "            ):\n",
    "                counts[\"below\"] += 1\n",
    "            else:\n",
    "                counts[\"equal\"] += 1\n",
    "\n",
    "    print(np.unique(d_names))\n",
    "    if len(p1) == 0:\n",
    "        continue\n",
    "\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "    plt.scatter(\n",
    "        p1,\n",
    "        p2,\n",
    "        c=[full_detector_colors[d] for d in d_names],\n",
    "        # marker=[full_detector_markers[d] for d in d_names],\n",
    "        alpha=0.7,\n",
    "        s=12,\n",
    "    )\n",
    "    custom_grid(plt.gca())\n",
    "    k_classif_str = k_classif.replace(\"_\", \"\\_\")\n",
    "    plt.xlabel(f\"{k_classif_str} of none baseline\")\n",
    "    plt.ylabel(f\"{k_classif_str} of detect + filter\")\n",
    "    plt.xscale(scale)\n",
    "    plt.yscale(scale)\n",
    "\n",
    "    xlims = plt.xlim()\n",
    "    ylims = plt.ylim()\n",
    "\n",
    "    rmin = min(xlims[0], ylims[0])\n",
    "    rmax = max(xlims[1], ylims[1])\n",
    "\n",
    "    plt.plot([rmin, rmax], [rmin, rmax], color=\"black\")\n",
    "\n",
    "    plt.plot([scale_f(rmin, True), rmax], [rmin, scale_f(rmax, False)], color=\"orange\")\n",
    "    plt.plot([rmin, scale_f(rmax, False)], [scale_f(rmin, True), rmax], color=\"orange\")\n",
    "\n",
    "    plt.xlim(rmin, rmax)\n",
    "    plt.ylim(rmin, rmax)\n",
    "\n",
    "    plt.title(f\"{counts['above']} / {counts['equal']} / {counts['below']}\")\n",
    "\n",
    "    tf.savefig(\n",
    "        f\"figures/detect_vs_none/{prefix.replace('/','_')}_{exp_alt}_{k_classif}\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "regr = make_pipeline(PolynomialFeatures(1), HuberRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "estim = prefix.split(\"/\")[-1]\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "r_none = all_results_cv[exp_alt][prefix][\"logl\"][\"oracl\"][\n",
    "    all_results_cv[exp_alt][prefix][\"logl\"][\"oracl\"][\"detector_name\"] == \"none\"\n",
    "]\n",
    "\n",
    "r_qualities = defaultdict(list)\n",
    "none_losses = defaultdict(list)\n",
    "\n",
    "for detect_name, dataset_name, ranking_quality in all_results_cv[exp_alt][prefix][\n",
    "    \"logl\"\n",
    "][\"oracl\"][[\"detector_name\", \"dataset_name\", \"global_ranking_quality\"]].itertuples(\n",
    "    index=False\n",
    "):\n",
    "\n",
    "    if detect_name in baselines + [\"random\"]:\n",
    "        continue\n",
    "\n",
    "    # if ranking_quality < 0.5:\n",
    "    #     continue\n",
    "\n",
    "    if estim != d_base_model_map[detect_name]:\n",
    "        continue\n",
    "\n",
    "    # if detect_name != 'agra': continue\n",
    "\n",
    "    assert len(r_none[r_none[\"dataset_name\"] == dataset_name]) == 1\n",
    "    logl_test_none = r_none[r_none[\"dataset_name\"] == dataset_name].iloc[0][\"logl_test\"]\n",
    "\n",
    "    r_qualities[detect_name].append(ranking_quality)\n",
    "    none_losses[detect_name].append(logl_test_none)\n",
    "\n",
    "\n",
    "tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "custom_grid(plt.gca())\n",
    "\n",
    "plt.xlabel(f\"test loss of none baseline\")\n",
    "plt.ylabel(f\"ranking quality\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "\n",
    "\n",
    "def t(x):\n",
    "    return np.log(x)\n",
    "\n",
    "\n",
    "for (d_name, r_quals), (_, none_los) in zip(r_qualities.items(), none_losses.items()):\n",
    "    plt.scatter(\n",
    "        none_los,\n",
    "        r_quals,\n",
    "        c=full_detector_colors[d_name],\n",
    "        marker=full_detector_markers[d_name],\n",
    "        alpha=0.7,\n",
    "        s=12,\n",
    "    )\n",
    "\n",
    "    none_los, r_quals = zip(*sorted(zip(none_los, r_quals)))\n",
    "    # plt.plot(none_los, r_quals, c=detector_colors[d_name], alpha=0.1)\n",
    "\n",
    "    regr.fit(t(np.array(none_los)[:, None]), r_quals)\n",
    "\n",
    "    x_plot = np.linspace(np.min(none_los), np.max(none_los))\n",
    "    plt.plot(\n",
    "        x_plot,\n",
    "        regr.predict(t(x_plot[:, None])),\n",
    "        c=full_detector_colors[d_name],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "# xlims = plt.xlim()\n",
    "# ylims = plt.ylim()\n",
    "\n",
    "# rmin = min(xlims[0], ylims[0])\n",
    "# rmax = max(xlims[1], ylims[1])\n",
    "\n",
    "# plt.plot([rmin, rmax], [rmin, rmax], color=\"black\")\n",
    "\n",
    "plt.ylim(0.5, 1)\n",
    "# plt.ylim(rmin, rmax)\n",
    "\n",
    "tf.savefig(f\"figures/perf_vs_detect/{prefix.replace('/','_')}_{exp_alt}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/gb\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "results_cv = all_results_cv[exp_alt][prefix][\"logl\"]\n",
    "\n",
    "df = (\n",
    "    results_cv[\"oracl\"]\n",
    "    .pivot(index=\"dataset_name\", columns=\"detector_name\", values=\"logl_test\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "diagram = Diagram(df.to_numpy(), treatment_names=df.columns, maximize_outcome=False)\n",
    "\n",
    "diagram.to_file(\n",
    "    f\"critdd_logl_test_{prefix.replace('/','_')}.pdf\",\n",
    "    alpha=0.05,\n",
    "    adjustment=\"holm\",\n",
    "    reverse_x=True,\n",
    "    axis_options={\"title\": \"critdd\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"gold\", \"silver\", \"none\", \"wood\"]\n",
    "\n",
    "res_oracl = (\n",
    "    results_cv[\"oracl\"]\n",
    "    .pivot(index=\"dataset_name\", columns=\"detector_name\", values=\"logl_test\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "res_clean = (\n",
    "    results_cv[\"clean\"]\n",
    "    .pivot(index=\"dataset_name\", columns=\"detector_name\", values=\"logl_test\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "res_noisy = (\n",
    "    results_cv[\"noisy\"]\n",
    "    .pivot(index=\"dataset_name\", columns=\"detector_name\", values=\"logl_test\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "cc = np.stack(\n",
    "    (res_oracl[cols].to_numpy(), res_noisy[cols].to_numpy(), res_clean[cols].to_numpy())\n",
    ")\n",
    "\n",
    "diagram = Diagrams(\n",
    "    cc.transpose((2, 1, 0)),\n",
    "    treatment_names=[\"oracle\", \"noisy\", \"clean\"],\n",
    "    maximize_outcome=False,\n",
    "    diagram_names=cols,\n",
    ")\n",
    "\n",
    "diagram.to_file(\n",
    "    f\"critdd_logl_test_onc_{prefix.replace('/','_')}.pdf\",\n",
    "    alpha=0.05,\n",
    "    adjustment=\"holm\",\n",
    "    reverse_x=True,\n",
    "    axis_options={\"title\": \"critdd\"},\n",
    ")\n",
    "\n",
    "\n",
    "diagram = Diagrams(\n",
    "    cc,\n",
    "    diagram_names=[\"oracle\", \"noisy\", \"clean\"],\n",
    "    maximize_outcome=False,\n",
    "    treatment_names=cols,\n",
    ")\n",
    "\n",
    "diagram.to_file(\n",
    "    f\"critdd_logl_test_comp_{prefix.replace('/','_')}.pdf\",\n",
    "    alpha=0.05,\n",
    "    adjustment=\"holm\",\n",
    "    reverse_x=True,\n",
    "    axis_options={\"title\": \"critdd\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = np.stack(\n",
    "    (res_oracl[cols].to_numpy(), res_noisy[cols].to_numpy(), res_clean[cols].to_numpy())\n",
    ").transpose((2, 1, 0))\n",
    "\n",
    "diagram = Diagrams(\n",
    "    cc,\n",
    "    treatment_names=[\"oracle\", \"noisy\", \"clean\"],\n",
    "    maximize_outcome=False,\n",
    "    diagram_names=cols,\n",
    ")\n",
    "\n",
    "diagram.to_file(\n",
    "    f\"critdd_logl_test_oracle_{prefix.replace('/','_')}.pdf\",\n",
    "    alpha=0.05,\n",
    "    adjustment=\"holm\",\n",
    "    reverse_x=True,\n",
    "    axis_options={\"title\": \"critdd\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = list(set(detectors) - set(baselines + [\"random\"]))\n",
    "\n",
    "cc = np.stack(\n",
    "    (\n",
    "        np.repeat([res_noisy[\"none\"].to_numpy()], len(competitors), axis=0).T,\n",
    "        np.repeat([res_clean[\"none\"].to_numpy()], len(competitors), axis=0).T,\n",
    "        res_oracl[competitors].to_numpy(),\n",
    "        res_noisy[competitors].to_numpy(),\n",
    "        res_clean[competitors].to_numpy(),\n",
    "    )\n",
    ").transpose((2, 1, 0))\n",
    "\n",
    "diagram = Diagrams(\n",
    "    cc,\n",
    "    treatment_names=[\"none noisy\", \"none clean\", \"oracle\", \"noisy\", \"clean\"],\n",
    "    maximize_outcome=False,\n",
    "    diagram_names=competitors,\n",
    ")\n",
    "\n",
    "diagram.to_file(\n",
    "    f\"critdd_logl_test_detectors_{prefix.replace('/','_')}.pdf\",\n",
    "    alpha=0.05,\n",
    "    adjustment=\"holm\",\n",
    "    reverse_x=True,\n",
    "    axis_options={\"title\": \"critdd\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining = dict()\n",
    "\n",
    "n = 12\n",
    "\n",
    "for ds, n_runs in (\n",
    "    all_results['byclass'][\"weak/klm\"].pivot_table(\n",
    "        index=\"detector_name\",\n",
    "        columns=\"dataset_name\",\n",
    "        values=\"estim_time\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    ").items():\n",
    "\n",
    "    # remaining[ds] = (8*12*10 - n_runs[np.logical_and(n_runs < 8*12*10, ~n_runs.isin([12, 120]))]).sum()\n",
    "    # remaining[ds] = (9*12 - n_runs[np.logical_and(n_runs <9*12, ~n_runs.isin([12, 120]))]).sum()\n",
    "    # remaining[ds] = int(\n",
    "    #     (\n",
    "    #         n * 12 * 10\n",
    "    #         - n_runs[np.logical_and(~n_runs.isin([12, 120]), n_runs < n * 12 * 10)]\n",
    "    #     ).sum()\n",
    "    # )\n",
    "\n",
    "    # remaining[ds] += 1440*2\n",
    "\n",
    "    remaining[ds] = int(16*1440+4*12+120 - n_runs.sum())\n",
    "\n",
    "    # if remaining[ds]> 0:\n",
    "    #     remaining[ds] += 100\n",
    "\n",
    "    # print(ds, n_runs, remaining[ds])\n",
    "    print(ds, remaining[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_rbf = [\n",
    "    \"bank-marketing\",\n",
    "    \"bioresponse\",\n",
    "    \"census\",\n",
    "    \"mushroom\",\n",
    "    \"phishing\",\n",
    "    \"spambase\",\n",
    "    \"basketball\",\n",
    "    \"commercial\",\n",
    "    \"tennis\",\n",
    "    \"cifar10\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = \"klm\"\n",
    "\n",
    "all_results['allclass'][f\"weak/{classif}\"].pivot_table(\n",
    "    index=\"dataset_name\",\n",
    "    columns=\"detector_name\",\n",
    "    values=\"estim_time\",\n",
    "    aggfunc=\"max\",\n",
    "    # aggfunc=lambda x: np.percentile(x, 15),\n",
    ")[\"white_gold\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in [\n",
    "#     \"bank-marketing\",\n",
    "#     \"bioresponse\",\n",
    "#     \"census\",\n",
    "#     \"mushroom\",\n",
    "#     \"phishing\",\n",
    "#     \"spambase\",\n",
    "#     \"basketball\",\n",
    "#     \"commercial\",\n",
    "#     \"tennis\",\n",
    "#     \"cifar10\",\n",
    "# ]:\n",
    "#     for k in [\"estim\", \"estim_byclass\", \"relabel\"]:\n",
    "#         for n in [\"weak\", \"noise\"]:\n",
    "#             print(f\"ls {k}/{n}/klm/*/{d}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in all_results_cv[\"allclass\"].keys():\n",
    "    d = all_results_cv[\"allclass\"][k][\"logl\"][\"clean\"]\n",
    "\n",
    "    print(k)\n",
    "    for d_name, detector_name, params_classifier, params_detector in d[\n",
    "        [\"dataset_name\", \"detector_name\", \"params_classifier\", \"params_detector\"]\n",
    "    ].itertuples(index=False):\n",
    "        if (\n",
    "            detector_name in d_base_model_map.keys()\n",
    "            and d_base_model_map[detector_name] == \"klm\"\n",
    "        ):\n",
    "            # print(d_base_model_map[detector_name] )\n",
    "            print(d_name, detector_name, params_detector, params_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"weak/klm\"\n",
    "exp_alt = \"allclass\"\n",
    "\n",
    "hp_pivot = all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"].pivot(\n",
    "    values=\"params_classifier\", index=\"dataset_name\", columns=\"detector_name\"\n",
    ")\n",
    "res = all_results[exp_alt][prefix]\n",
    "res_none = all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"][\n",
    "    all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"][\"detector_name\"] == \"none\"\n",
    "][[\"dataset_name\", \"logl_test\"]].set_index(\"dataset_name\")\n",
    "res_silver = all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"][\n",
    "    all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"][\"detector_name\"] == \"silver\"\n",
    "][[\"dataset_name\", \"logl_test\"]].set_index(\"dataset_name\")\n",
    "res_gold = all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"][\n",
    "    all_results_cv[exp_alt][prefix][\"logl\"][\"clean\"][\"detector_name\"] == \"gold\"\n",
    "][[\"dataset_name\", \"logl_test\"]].set_index(\"dataset_name\")\n",
    "\n",
    "mult = 1.5\n",
    "fig, axes = plt.subplots(4, 5, figsize=(12 * mult, 8 * mult))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    axis = axes[i // 5][i % 5]\n",
    "\n",
    "    for detector in set(detectors) - {\"gb_consensus\", \"klm_consensus\"} - set(baselines):\n",
    "        hp_this = hp_pivot.loc[dataset][detector]\n",
    "        logl_this = res[(res.dataset_name == dataset) & (res.detector_name == detector)]\n",
    "        logl_this = logl_this[logl_this.params_classifier.apply(lambda x: x == hp_this)]\n",
    "\n",
    "        split_quantiles = []\n",
    "        logls = []\n",
    "        for p_split, logl in logl_this[[\"params_splitter\", \"logl_test\"]].itertuples(\n",
    "            index=False\n",
    "        ):\n",
    "            split_quantiles.append(p_split[\"quantile\"])\n",
    "            logls.append(logl)\n",
    "\n",
    "        axis.plot(split_quantiles, logls, label=detector)\n",
    "\n",
    "    axis.plot([0, 1], [res_none.loc[dataset].values] * 2, color=\"red\")\n",
    "    axis.plot([0, 1], [res_silver.loc[dataset].values] * 2, color=\"silver\")\n",
    "    axis.plot([0, 1], [res_gold.loc[dataset].values] * 2, color=\"gold\")\n",
    "\n",
    "    custom_grid(axis)\n",
    "    axis.set_title(dataset)\n",
    "    # axis.legend()\n",
    "    axis.set_xlabel(\"split quantile\")\n",
    "    axis.set_ylabel(\"log loss test\")\n",
    "    axis.set_xlim()\n",
    "    axis.set_yscale(\"log\")\n",
    "\n",
    "handles, labels = axes[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_dirs = [\n",
    "    (\n",
    "        \"weak\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/detect/weak\"),\n",
    "    ),\n",
    "    (\n",
    "        \"noise\",\n",
    "        os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/detect/noise\"),\n",
    "    ),\n",
    "]\n",
    "\n",
    "all_detect = dict()\n",
    "for prefix, dir in detect_dirs:\n",
    "    results_ = []\n",
    "    methods = os.listdir(dir)\n",
    "\n",
    "    for method in methods:\n",
    "        for fname in os.listdir(os.path.join(dir, method)):\n",
    "            dataset, ext = fname.split(\".\")\n",
    "            if ext != \"json\":\n",
    "                continue\n",
    "\n",
    "            with open(os.path.join(dir, method, f\"{dataset}.json\")) as f:\n",
    "                results_.append(pd.read_json(f, orient=\"records\"))\n",
    "\n",
    "    results_ = pd.concat(results_)\n",
    "\n",
    "    all_detect[prefix] = results_.reset_index(drop=True)\n",
    "    all_detect[prefix][\"detect_run\"] = all_detect[prefix].groupby([\"dataset_name\", \"detector_name\"]).cumcount()+1\n",
    "\n",
    "for k, v in all_results.items():\n",
    "    v.loc[pd.isna(v[\"params_detector\"]), \"params_detector\"] = v.loc[\n",
    "        pd.isna(v[\"params_detector\"]), \"params\"\n",
    "    ]\n",
    "    v.drop(\"params\", inplace=True, axis=1)\n",
    "\n",
    "\n",
    "all_merged = dict()\n",
    "for prefix in [\"weak\", \"noise\"]:\n",
    "    all_results[prefix + \"/klm\"][\"params_detector\"] = all_results[prefix + \"/klm\"][\n",
    "        \"params_detector\"\n",
    "    ].astype(str)\n",
    "    all_detect[prefix][\"params\"] = all_detect[prefix][\"params\"].astype(str)\n",
    "\n",
    "    all_merged[prefix] = pd.merge(\n",
    "        all_results[prefix + \"/klm\"],\n",
    "        all_detect[prefix],\n",
    "        left_on=[\"dataset_name\", \"detector_name\", \"params_detector\"],\n",
    "        right_on=[\"dataset_name\", \"detector_name\", \"params\"],\n",
    "    )\n",
    "\n",
    "    all_merged[prefix][\"params_detector\"] = all_merged[prefix][\"params_detector\"].apply(\n",
    "        eval\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_path = os.path.join(os.path.expanduser(\"~\"), f\"{output_dir}/detect\")\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "class TrustScoreReader:\n",
    "\n",
    "    def __init__(self, base_path, dataset, detector):\n",
    "\n",
    "        with open(os.path.join(base_path, detector, f\"{dataset}.json\"), mode=\"r\") as f:\n",
    "            self.results_json = json.load(f)\n",
    "        self.results_hdf5 = h5py.File(\n",
    "            os.path.join(base_path, detector, f\"{dataset}.hdf5\"), \"r\"\n",
    "        )\n",
    "\n",
    "        assert len(self.results_hdf5[\"trust_scores\"]) == len(self.results_json)\n",
    "\n",
    "    def get(self, i):\n",
    "        return self.results_json[i], self.results_hdf5[f\"trust_scores/{i}\"][...]\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.results_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_weak_datasets, datasets_ranked_by_time\n",
    "import os\n",
    "\n",
    "datasets_folder = os.path.join(os.path.expanduser(\"~\"), \"datasets\")\n",
    "\n",
    "datasets = list(\n",
    "    filter(\n",
    "        lambda dataset: dataset not in [\"imdb136\", \"cifar10\"], datasets_ranked_by_time\n",
    "    )\n",
    ")\n",
    "\n",
    "weak_datasets = get_weak_datasets(\n",
    "    cache_folder=datasets_folder,\n",
    "    corruption=\"weak\",\n",
    "    seed=1,\n",
    "    datasets=datasets,\n",
    ")\n",
    "\n",
    "noise_datasets = get_weak_datasets(\n",
    "    cache_folder=datasets_folder,\n",
    "    corruption=\"noise\",\n",
    "    seed=1,\n",
    "    datasets=datasets_ranked_by_time,\n",
    ")\n",
    "\n",
    "loaded_datasets = dict(weak=weak_datasets, noise=noise_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def plot_class_balance(prefix, reference=\"clean\", k=\"oracl\"):\n",
    "\n",
    "    mapping = {\"oracl\": \"test\", \"clean\": \"val\", \"noisy\": \"noisy_val\"}\n",
    "    best_cv = (\n",
    "        all_merged[prefix]\n",
    "        .groupby([\"dataset_name\", \"detector_name\"])[f\"logl_{mapping[k]}\"]\n",
    "        .idxmin()\n",
    "    )\n",
    "    best_trust_scores_index = (\n",
    "        all_merged[prefix]\n",
    "        .iloc[best_cv]\n",
    "        .pivot(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"detect_run\",\n",
    "        )\n",
    "    )\n",
    "    best_quantile = (\n",
    "        all_merged[prefix]\n",
    "        .iloc[best_cv]\n",
    "        .pivot(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"params_splitter\",\n",
    "        )\n",
    "        .map(lambda dict: dict.get(\"quantile\"))\n",
    "    )\n",
    "    best_threshold = (\n",
    "        all_merged[prefix]\n",
    "        .iloc[best_cv]\n",
    "        .pivot(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"params_splitter\",\n",
    "        )\n",
    "        .map(lambda dict: dict.get(\"threshold\"))\n",
    "    )\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    top = 0\n",
    "    bottom = 0\n",
    "    for detector_name, dataset_name in product(\n",
    "        best_trust_scores_index.index, best_trust_scores_index.columns\n",
    "    ):\n",
    "        trust_score_reader = TrustScoreReader(\n",
    "            os.path.join(detect_path, prefix), dataset_name, detector_name\n",
    "        )\n",
    "        trust_scores = trust_score_reader.get(\n",
    "            best_trust_scores_index.loc[detector_name, dataset_name] - 1\n",
    "        )[1]\n",
    "        dataset = loaded_datasets[prefix][dataset_name]\n",
    "        y_train_clean = dataset[\"train\"][\"target\"]\n",
    "        y_train_noisy = dataset[\"train\"][\"noisy_target\"]\n",
    "        y_test = dataset[\"test\"][\"target\"]\n",
    "        unlabeled = y_train_noisy == -1\n",
    "        y_train_clean = np.asarray(y_train_clean)[~unlabeled]\n",
    "        y_train_noisy = np.asarray(y_train_noisy)[~unlabeled]\n",
    "        n_classes = len(np.unique(y_test))\n",
    "        if \"consensus\" in detector_name:\n",
    "            trusted = trust_scores >= best_threshold.loc[detector_name, dataset_name]\n",
    "        else:\n",
    "            trusted = trust_scores >= np.quantile(\n",
    "                trust_scores, q=best_quantile.loc[detector_name, dataset_name]\n",
    "            )\n",
    "        prior_trusted = np.bincount(y_train_noisy[trusted], minlength=n_classes) / len(\n",
    "            y_train_noisy[trusted]\n",
    "        )\n",
    "        prior_untrusted = np.bincount(\n",
    "            y_train_noisy[~trusted], minlength=n_classes\n",
    "        ) / len(y_train_noisy[~trusted])\n",
    "        prior_clean = np.bincount(y_test, minlength=n_classes) / len(y_test)\n",
    "        prior_noisy = np.bincount(y_train_noisy, minlength=n_classes) / len(\n",
    "            y_train_noisy\n",
    "        )\n",
    "        x_clean = np.min(prior_clean) / np.max(prior_clean)\n",
    "        x_noisy = np.min(prior_noisy) / np.max(prior_noisy)\n",
    "        x = x_clean if reference == \"clean\" else x_noisy\n",
    "        y = np.min(prior_trusted) / np.max(prior_trusted)\n",
    "        plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            color=\"tab:blue\" if x_noisy > x_clean else \"tab:orange\",\n",
    "            # marker=full_detector_markers[detector_name],\n",
    "            alpha=0.7,\n",
    "            s=12,\n",
    "        )\n",
    "        top += np.sum(y > x)\n",
    "        bottom += np.sum(y <= x)\n",
    "\n",
    "    plt.ylabel(\"filtered class balance\")\n",
    "    if reference == \"clean\":\n",
    "        plt.xlabel(\"clean class balance\")\n",
    "    else:\n",
    "        plt.xlabel(\"noisy class balance\")\n",
    "\n",
    "    # plt.title(f\"{top}/{bottom}\")\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(np.linspace(0, 1, 1000), np.linspace(0, 1, 1000), color=\"black\")\n",
    "\n",
    "    plt.xticks(np.arange(0, 1, 0.1), [], minor=True)\n",
    "    plt.yticks(np.arange(0, 1, 0.1), [], minor=True)\n",
    "    plt.xticks([0, 1], [0, 1], minor=False)\n",
    "    plt.yticks([0, 1], [0, 1], minor=False)\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     [], [], color=\"tab:blue\", label=f\"more bal. than {mapping[k][0]}.\", s=12\n",
    "    # )\n",
    "    # plt.scatter(\n",
    "    #     [], [], color=\"tab:orange\", label=f\"less bal. than {mapping[k][0]}.\", s=12\n",
    "    # )\n",
    "    # plt.legend()\n",
    "\n",
    "    tf.savefig(f\"figures/class_balance/{prefix.replace('/','_')}_{reference}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if not os.path.exists(\"figures/class_balance\"):\n",
    "    os.mkdir(\"figures/class_balance\")\n",
    "plot_class_balance(\"noise\", reference=\"clean\")\n",
    "plot_class_balance(\"noise\", reference=\"noisy\")\n",
    "plot_class_balance(\"weak\", reference=\"clean\")\n",
    "plot_class_balance(\"weak\", reference=\"noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def plot_rank_vs_perf(prefix, k):\n",
    "    logl_cv_allclass = all_val_cv[\"allclass\"][prefix][\"logl\"][k]\n",
    "    mapping = {\"oracl\": \"test\", \"clean\": \"val\", \"noisy\": \"noisy_val\"}\n",
    "    max_rank_global = (\n",
    "        all_results[prefix]\n",
    "        .iloc[\n",
    "            all_results[prefix]\n",
    "            .groupby([\"dataset_name\", \"detector_name\"])[f\"logl_{mapping[k]}\"]\n",
    "            .idxmin()\n",
    "        ]\n",
    "        .pivot(\n",
    "            index=\"detector_name\",\n",
    "            columns=\"dataset_name\",\n",
    "            values=\"global_ranking_quality\",\n",
    "        )\n",
    "    )\n",
    "    tf.figure(width=TMLR_textwidth * (2 / 5), ratio=1, pad=0.5)\n",
    "\n",
    "    list_perf_allclass = []\n",
    "    list_perf_rank = []\n",
    "\n",
    "    # plt.axvline(0.5, color=detector_colors[\"random\"], alpha=0.7)\n",
    "\n",
    "    for detector in set(detectors) - set(baselines) - set([\"random\"]):\n",
    "\n",
    "        perf_allclass = (\n",
    "            logl_cv_allclass.loc[logl_cv_allclass.index == detector].values\n",
    "            - logl_cv_allclass.loc[logl_cv_allclass.index == \"none\"].values\n",
    "        ) / (\n",
    "            logl_cv_allclass.loc[logl_cv_allclass.index == \"silver\"].values\n",
    "            - logl_cv_allclass.loc[logl_cv_allclass.index == \"none\"].values\n",
    "            + 1e-10\n",
    "        )\n",
    "        perf_allclass = (2 - perf_allclass) * 100\n",
    "        perf_rank = max_rank_global.loc[\n",
    "            max_rank_global.index == detector\n",
    "        ].values.ravel()\n",
    "\n",
    "        plt.scatter(\n",
    "            perf_rank,\n",
    "            perf_allclass,\n",
    "            color=full_detector_colors[detector],\n",
    "            marker=full_detector_markers[detector],\n",
    "            s=12,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        \n",
    "        list_perf_allclass.append(perf_allclass.ravel())\n",
    "        list_perf_rank.append(perf_rank)\n",
    "\n",
    "    plt.xlabel(\"ranking quality\")\n",
    "    plt.ylabel(\"normalized test log loss\")\n",
    "\n",
    "    custom_grid(plt.gca())\n",
    "\n",
    "    plt.xlim(0.5, 1)\n",
    "    plt.ylim(0, 300)\n",
    "\n",
    "    y = np.concatenate(list_perf_allclass)\n",
    "    X = np.concatenate(list_perf_rank).reshape(-1, 1)\n",
    "    y = y[X.ravel() >= 0.5]\n",
    "    X = X[X.ravel() >= 0.5]\n",
    "    lr = HuberRegressor().fit(X, y)\n",
    "    print(lr.coef_)\n",
    "    print(r2_score(y, lr.predict(X)))\n",
    "\n",
    "    x = np.linspace(0.5, 1, 1000)\n",
    "    plt.plot(x, lr.predict(x.reshape(-1, 1)), color=\"black\", alpha=0.7)\n",
    "\n",
    "    plt.xticks(np.arange(0.5, 1, 0.1), [], minor=True)\n",
    "    plt.yticks(np.arange(0, 300, 25), [], minor=True)\n",
    "    plt.xticks([0.5, 1], [0.5, 1], minor=False)\n",
    "    plt.yticks([100, 200], [100, 200], minor=False)\n",
    "\n",
    "    tf.savefig(f\"figures/rank_vs_perf/{prefix.replace('/','_')}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if not os.path.exists(\"figures/rank_vs_perf\"):\n",
    "    os.makedirs(\"figures/rank_vs_perf\")\n",
    "\n",
    "plot_rank_vs_perf(prefix=\"noise/klm\", k=\"oracl\")\n",
    "plot_rank_vs_perf(prefix=\"weak/klm\", k=\"oracl\")\n",
    "plot_rank_vs_perf(prefix=\"noise/gb\", k=\"oracl\")\n",
    "plot_rank_vs_perf(prefix=\"weak/gb\", k=\"oracl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
